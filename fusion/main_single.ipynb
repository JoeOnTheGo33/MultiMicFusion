{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "import noisereduce as nr\n",
    "import torch as tr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, sr = torchaudio.load('../data/sensor_1.wav')\n",
    "x2, sr = torchaudio.load('../data/sensor_2.wav')\n",
    "x3, sr = torchaudio.load('../data/sensor_3.wav')\n",
    "X = tr.concat([x1], dim=0)\n",
    "\n",
    "N = len(x1[0])\n",
    "n = 3\n",
    "tt = tr.arange(N) / sr\n",
    "ii = tr.linspace(0, N, n, dtype=tr.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_or(a, w):\n",
    "    w = int(w)\n",
    "    y = tr.zeros(a.shape)\n",
    "    for i in range(w, a.shape[0]-w , w//4):\n",
    "        y[i-w:i+w] = True in a[i-w:i+w]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms = lambda x: tr.sqrt(tr.mean(x**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(samples, window_size):\n",
    "    # samples: (..., time)\n",
    "    y = []\n",
    "    for j in range(samples.shape[0]):\n",
    "        for i in range(-window_size//2, window_size//2):\n",
    "            y.append(tr.roll(samples[j], i, dims=0))\n",
    "    return tr.mean(tr.row_stack(y), dim=0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windows(length, window_size):\n",
    "    for j in range(0, length, window_size):\n",
    "        yield j, j + window_size - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Vocal Frequency Band Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://seaindia.in/blogs/human-voice-frequency-range/\n",
    "X = torchaudio.functional.lowpass_biquad(X, sr, 3500)\n",
    "X = torchaudio.functional.highpass_biquad(X, sr, 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Amplitude Segmention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22675736961451248\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters:\n",
    "ease_l = 10000\n",
    "print(10000 / sr)\n",
    "threshold = 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = []\n",
    "\n",
    "for x in X: # for each sensor\n",
    "    E = tr.sqrt(tr.mean(x**2)) # Calculate a baseline\n",
    "    c = tr.abs(x) > E * threshold\n",
    "    c = long_or(c, .1 * sr) # Expand acceptance bands\n",
    "\n",
    "    ease_in = tr.where(tr.diff(c) > 0)[0] # Find rises\n",
    "    ease_out = tr.where(tr.diff(c) < 0)[0] # Find falls\n",
    "    \n",
    "    # Assign linear fade-ins at rises\n",
    "    for i in ease_in:\n",
    "        ii = tr.arange(max(0, i-ease_l), min(i+ease_l, len(c)))\n",
    "        c[ii] = (ii - ii[0]) / len(ii)\n",
    "    # Assign linear fade-outs at falls\n",
    "    for i in ease_out:\n",
    "        ii = tr.arange(max(0, i-ease_l), min(i+ease_l, len(c)))\n",
    "        c[ii] = 1 - (ii - ii[0]) / len(ii)\n",
    "\n",
    "    C.append(c)\n",
    "\n",
    "C = tr.vstack(C)\n",
    "# C : (3, N), cross faded weights that accept louder segments per sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of noise per sensor: 13.37%\n"
     ]
    }
   ],
   "source": [
    "print(\"Percent of noise per sensor:\", \", \".join([f\"{1 - tr.sum(c) / len(c):2.2%}\" for c in C]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use volume discriminator to separate high volume and low volume sounds (segmentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentation\n",
    "X_high = X * C\n",
    "iC = 1 - C\n",
    "X_low = X * iC\n",
    "\n",
    "X_low /= rms(X_low) * 10 # Standardize noise volume\n",
    "\n",
    "X_noise = []\n",
    "for i in range(X_low.shape[0]):\n",
    "    x = X_low[i][iC[i] > .5] # Truncate silence in each noise sample\n",
    "    x /= rms(x) * 10 # Normalize noise per sensor\n",
    "    X_noise.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export each segment for evaluation\n",
    "torchaudio.save('high_segment.wav', X_high, sr)\n",
    "torchaudio.save('low_segment.wav', tr.unsqueeze(tr.cat(X_noise), 0), sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Crossed Noise Reduction\n",
    "Cross apply noise samples. If the noise sample and clean sample come from the same sensor more of the noise is removed. Noise samples are applied to other sensors to a lessor degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clean = X_high.clone()\n",
    "\n",
    "for i in range(1):\n",
    "    # Cross apply each noise sample onto each vocal sample. Noise applied sequentially\n",
    "    for j in range(1):\n",
    "        X_clean[i] = tr.tensor(nr.reduce_noise(y=X_clean[i], y_noise=X_noise[j],\n",
    "                                               sr=sr,\n",
    "                                               prop_decrease=.9 if i == j else .75,\n",
    "                                               n_std_thresh_stationary=.5,\n",
    "                                               stationary=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Fusion\n",
    "Weight sensors by RMS amplitude (volume) and put into moving average fusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]])\n",
      "torch.Size([1, 9595771])\n"
     ]
    }
   ],
   "source": [
    "W = tr.reshape(tr.sqrt(tr.mean(X_clean**2, axis=1)), (1,1))\n",
    "W /= W.sum()\n",
    "print(W)\n",
    "print(X_clean.shape)\n",
    "x_clean = moving_average(X_clean * W, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Volume Leveling\n",
    "Attempt volume regularization. i.e. making the quiet segments of the cleaned signal louder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input RMS: tensor(0.0094)\n",
      "tensor(0.0562)\n"
     ]
    }
   ],
   "source": [
    "def soft_limit(x, L):\n",
    "    # L = np.log10(L / 20)\n",
    "    return 2 / (1 + tr.exp(-x / L)) - 1\n",
    "\n",
    "print(\"Input RMS:\", rms(x_clean))\n",
    "x_clean = soft_limit(x_clean, .005)\n",
    "x_clean = torchaudio.transforms.Vol(-25, 'db')(x_clean  / rms(x_clean))\n",
    "print(rms(x_clean))\n",
    "x_clean = torchaudio.functional.lowpass_biquad(x_clean, sr, 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchaudio.save('output_single.wav', x_clean, sr, encoding=\"PCM_S\", bits_per_sample=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
