{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import noisereduce as nr\n",
    "import torch as tr\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, sr = torchaudio.load('../normalized/sensor_1.wav')\n",
    "x2, sr = torchaudio.load('../normalized/sensor_2.wav')\n",
    "x3, sr = torchaudio.load('../normalized/sensor_3.wav')\n",
    "X = tr.concat([x1, x2, x3], dim=0)\n",
    "\n",
    "N = len(x1[0])\n",
    "n = 3\n",
    "tt = tr.arange(N) / sr\n",
    "ii = tr.linspace(0, N, n, dtype=tr.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_or(a, w):\n",
    "    w = int(w)\n",
    "    y = tr.zeros(a.shape)\n",
    "    for i in range(w, a.shape[0]-w , w//4):\n",
    "        y[i-w:i+w] = True in a[i-w:i+w]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms = lambda x: tr.sqrt(tr.mean(x**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(samples, window_size):\n",
    "    # samples: (..., time)\n",
    "    y = []\n",
    "    for j in range(samples.shape[0]):\n",
    "        for i in range(-window_size//2, window_size//2):\n",
    "            y.append(tr.roll(samples[j], i, dims=0))\n",
    "    return tr.mean(tr.row_stack(y), dim=0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windows(length, window_size):\n",
    "    for j in range(0, length, window_size):\n",
    "        yield j, j + window_size - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Range Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://seaindia.in/blogs/human-voice-frequency-range/\n",
    "X = torchaudio.functional.lowpass_biquad(X, sr, 3500)\n",
    "X = torchaudio.functional.highpass_biquad(X, sr, 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amplitude Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "C = []\n",
    "ease_l = 10000\n",
    "threshold = 1.2\n",
    "\n",
    "for x in X: # for each sensor\n",
    "    E = tr.sqrt(tr.mean(x**2)) # Calculate a baseline\n",
    "    c = tr.abs(x) > E * threshold\n",
    "    c = long_or(c, .1 * sr) # expand acceptance bands\n",
    "\n",
    "    ease_in = tr.where(tr.diff(c) > 0)[0]\n",
    "    ease_out = tr.where(tr.diff(c) < 0)[0]\n",
    "    \n",
    "    for i in ease_in:\n",
    "        ii = tr.arange(max(0, i-ease_l), min(i+ease_l, len(c)))\n",
    "        c[ii] = (ii - ii[0]) / len(ii)\n",
    "    \n",
    "    for i in ease_out:\n",
    "        ii = tr.arange(max(0, i-ease_l), min(i+ease_l, len(c)))\n",
    "        c[ii] = 1 - (ii - ii[0]) / len(ii)\n",
    "\n",
    "    C.append(c)\n",
    "    # plt.fill_between(tt, c, alpha=.3)\n",
    "    # plt.yticks([])\n",
    "C = tr.vstack(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusion\n",
    "Use volume discriminator to separate high volume and low volume sounds (segmentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentation\n",
    "X_high = X * C\n",
    "iC = 1 - C\n",
    "X_low = X * iC\n",
    "\n",
    "X_low /= rms(X_low) * 10\n",
    "\n",
    "X_noise = []\n",
    "for i in range(3):\n",
    "    x = X_low[i][iC[i] > .5]\n",
    "    x /= rms(x) * 10\n",
    "    X_noise.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchaudio.save('high_segment.wav', X_high, sr)\n",
    "torchaudio.save('low_segment.wav', tr.unsqueeze(tr.cat(X_noise), 0), sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross apply noise segments and use moving average as fusion function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clean = X_high.clone()\n",
    "\n",
    "for i in range(3):\n",
    "    # Cross apply each noise sample onto each vocal sample. Noise applied sequentially\n",
    "    for j in range(3):\n",
    "        X_clean[i] = tr.tensor(nr.reduce_noise(y=X_clean[i], y_noise=X_noise[j],\n",
    "                                               sr=sr,\n",
    "                                               prop_decrease=.6 if i == j else .4,\n",
    "                                               thresh_n_mult_nonstationary=2,\n",
    "                                               stationary=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5465],\n",
      "        [0.3570],\n",
      "        [0.0966]])\n",
      "torch.Size([3, 9595771])\n"
     ]
    }
   ],
   "source": [
    "W = tr.reshape(tr.sqrt(tr.mean(X_clean**2, axis=1)), (3,1))\n",
    "W /= W.sum()\n",
    "print(W)\n",
    "print(X_clean.shape)\n",
    "x_clean = moving_average(X_clean * W, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt volume regularization. i.e. making the quiet segments of the cleaned signal louder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x_clean.clone()\n",
    "for i, j in windows(N, int(.1 * sr)):\n",
    "    i -= int(.05 * sr)\n",
    "    j += int(.05 * sr)\n",
    "    y[i:j] /= rms(y[i:j])\n",
    "x_clean = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchaudio.save('output.wav', x_clean * .1, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
